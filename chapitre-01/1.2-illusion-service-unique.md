# 1.2 L'illusion du service unique

> *"Un conteneur pour les gouverner tous, un conteneur pour les trouver, un conteneur pour les amener tous et dans les ténèbres les lier."*

## Le mythe du monolithe conteneurisé

Après le succès du premier `docker run`, vient l'illusion suprême : **une application = un conteneur**. Pourquoi compliquer quand un seul conteneur suffit ?

Cette phase est particulièrement dangereuse car elle semble parfaitement logique. Regardons pourquoi.

## L'analogie de la boîte à outils

Imaginez que vous partiez en randonnée. Vous pourriez emporter :
- Un sac à dos séparé pour la nourriture
- Un autre pour les vêtements
- Un troisième pour l'équipement de camping
- Un quatrième pour la pharmacie

Mais c'est lourd et compliqué. Alors vous achetez **UNE** boîte magique qui contient tout : nourriture, vêtements, tente, médicaments, tout organisé parfaitement.

Le conteneur unique, c'est cette boîte magique. Au lieu de gérer plusieurs services (web server, base de données, cache, worker queue...), tout va dans un seul conteneur qui "contient tout".

## Les arguments apparemment solides

### Argument 1 : La simplicité de déploiement

"Un conteneur = une commande à déployer"

- Pas besoin d'orchestrer plusieurs services
- Pas de problèmes de dépendances entre conteneurs
- Déploiement atomique : tout ou rien

### Argument 2 : La cohérence environnementale

"Tout fonctionne ensemble ou rien ne fonctionne"

- Plus de versions incompatibles entre services
- Pas de "ça marche en staging mais pas en prod"
- Reproduction exacte de l'environnement

### Argument 3 : L'optimisation des ressources

"Moins de conteneurs = moins de surcharge"

- Un seul processus = moins de RAM consommée
- Pas de communication inter-conteneurs
- Image plus légère

### Argument 4 : La facilité de développement

"Le développeur contrôle tout"

- Pas besoin de comprendre comment les services communiquent
- Debug plus simple : tout est dans le même conteneur
- Logs consolidés

## Les premiers signes de craquelure

Pourtant, même dans cette approche apparemment parfaite, des fissures apparaissent.

### Problème 1 : Les logs mélangés

Votre application web, votre worker de fond, votre cron job : tous écrivent dans stdout. Résultat :
- Logs impossibles à filtrer
- Debugging cauchemardesque
- Métriques mélangées

### Problème 2 : Les ressources partagées

Un service qui consomme beaucoup de CPU affecte tous les autres. Pas de réelle isolation.

### Problème 3 : Les redémarrages coûteux

Un bug dans le worker ? Tout l'application redémarre.

### Problème 4 : L'évolutivité nulle

Besoin de scaler le frontend mais pas le worker ? Impossible.

## L'exemple concret : l'application "simple"

Prenons une application web classique : blog avec commentaires.

**Architecture "idéale" avec un conteneur unique :**

```dockerfile
FROM node:16-alpine

# Installer tous les outils
RUN apk add --no-cache postgresql-client redis

# Copier le code
COPY . /app
WORKDIR /app

# Installer les dépendances
RUN npm install

# Script de démarrage qui lance tout
COPY start.sh /start.sh
RUN chmod +x /start.sh

CMD ["/start.sh"]
```

Le `start.sh` :
```bash
#!/bin/sh
# Lancer PostgreSQL en arrière-plan
postgres &

# Lancer Redis
redis-server &

# Attendre que les services soient prêts
sleep 5

# Lancer l'application
npm start &

# Lancer le worker de commentaires
npm run worker &

# Attendre indéfiniment
wait
```

## Pourquoi ça semble marcher (au début)

Cette approche fonctionne étonnamment bien pour :

- **Les prototypes** : "Vite fait, bien fait"
- **Les MVP** : "On verra pour la scalabilité plus tard"
- **Les applications personnelles** : "Pas besoin d'optimiser"
- **Les démos** : "Ça impressionne le client"

Le conteneur se lance, tout fonctionne, les utilisateurs sont contents.

## La dette technique accumulée

Mais chaque jour d'utilisation ajoute de la dette :

### Dette 1 : La complexité du script de démarrage

Le `start.sh` devient un monstre :
```bash
#!/bin/bash
# Lancer postgres
postgres &
POSTGRES_PID=$!

# Attendre que postgres soit prêt
until pg_isready -h localhost; do sleep 1; done

# Lancer redis
redis-server &
REDIS_PID=$!

# Lancer l'app
npm start &
APP_PID=$!

# Lancer le worker
npm run worker &
WORKER_PID=$!

# Fonction de nettoyage
cleanup() {
    kill $APP_PID $WORKER_PID $REDIS_PID $POSTGRES_PID
}
trap cleanup SIGTERM

# Attendre
wait
```

### Dette 2 : Les volumes impossibles à gérer

Où stocker les données PostgreSQL ? Les uploads des utilisateurs ? Les sessions Redis ?

### Dette 3 : Les mises à jour risquées

Toute mise à jour nécessite de reconstruire l'ENTIÈRE image.

### Dette 4 : Le monitoring impossible

Comment monitorer individuellement chaque service ?

## Le moment du réveil

L'illusion se brise généralement lors d'un incident :

- **Incident 1** : Le worker de commentaires crash, faisant redémarrer toute l'app
- **Incident 2** : Besoin de scaler le frontend pendant un pic de trafic
- **Incident 3** : Un bug en prod nécessite un redémarrage rapide du seul worker
- **Incident 4** : Les logs deviennent illisibles avec 1000 utilisateurs

C'est là que naît la prise de conscience : **un conteneur ne peut pas tout faire**.

## La leçon apprise (trop tard)

Le conteneur unique n'est pas une architecture ; c'est une excuse pour éviter l'architecture. Il reporte les décisions difficiles plutôt que de les résoudre.

Mais cette prise de conscience vient généralement après des mois d'accumulation de dette technique, quand refactorer devient une montagne insurmontable.

---

*"L'illusion du service unique n'est pas une erreur technique. C'est une erreur cognitive : croire que la simplicité peut remplacer la séparation des responsabilités."*

---

[Section suivante : 1.3 Docker Compose](./1.3-docker-compose.md) | [Section précédente : 1.1 Le premier docker run](./1.1-premier-docker-run.md)
